{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision trees project_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themikepeng/projecttitanicdecisiontrees/blob/main/decision_trees_project_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h2p12912aix"
      },
      "source": [
        "#Project Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Tjiw16064a"
      },
      "source": [
        "*Copyright: © 2020 NexStream Technical Institute, LLC*  \n",
        "*Author:  Michael Peng*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxACAYud1Eej"
      },
      "source": [
        "#Drive Setup\n",
        "1.  Create a folder called `decision trees` under your `Colab Notebooks` folder\n",
        "2.  Copy this reference notebook and the data files (`titanic_testing_data.csv` and `titanic_training.csv`) to the `decision trees` folder\n",
        "3. Mount your Google Drive:\n",
        "  \n",
        "  a.  Run the cell below\n",
        "  \n",
        "  b.  Go to the URL given and sign into your Google account\n",
        "\n",
        "  c.  Copy and enter the authorization code into the textbox under the cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6xplFrp0qU1",
        "outputId": "06cd74db-cc77-4690-bef6-2540c6a94713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1oL7IF9-rme"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L5u5t_ATBi6",
        "outputId": "5502efed-2512-4f47-acc1-b6d30121dfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#graphviz is a nice library that will allow us to visualize our decision tree\n",
        "import sys\n",
        "!{sys.executable} -m pip install graphviz\n",
        "from graphviz import Digraph\n",
        "import os\n",
        "#used in preprocessing to find the most common categorical terms and their frequencies\n",
        "from collections import Counter\n",
        "#NumPy: numerical and matrix tools\n",
        "import numpy as np\n",
        "#used in split between training and validation sets\n",
        "import sklearn.model_selection\n",
        "#used to import .csv data files\n",
        "from numpy import genfromtxt\n",
        "#conveniently performs mode (preprocessing, fit) and entropy calculations\n",
        "from scipy import stats\n",
        "\n",
        "#a very small number \n",
        "eps = 1e-5"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLtmZfZKJAB"
      },
      "source": [
        "# Intro to the Titanic dataset\n",
        "\n",
        "**Goal: we want to implement a decision tree model that can predict whether a passenger on the Titanic survived or didn't survive**\n",
        "*  The Titanic dataset is split between training data and test data\n",
        "  *  The training data will then be further split into training and validation sets\n",
        "  *  The test data does not contain the true class labels: we don't know if these passengers survived or not\n",
        "  *  It is good practice to not predict on the test set until we've finalized our model!\n",
        "*  We will then train an instance of this decision tree on the training set (sample points), and ensure that it is generalizable to the validation set\n",
        "*  Finally, we will make our predictions on the test set\n",
        "\n",
        "*  The Titanic dataset has the following fields:\n",
        "\n",
        "1. survived: the label we want to predict; 1 indicates the person survived, whereas 0 indicates the person\n",
        "died\n",
        "2. pclass: Measure of socioeconomic status; 1 is upper, 2 is middle, 3 is lower\n",
        "4. sex: Male/female\n",
        "3. age: Fractional if less than 1\n",
        "5. sibsp: Number of siblings/spouses aboard the Titanic\n",
        "6. parch: Number of parents/children aboard the Titanic\n",
        "7. ticket: Ticket number\n",
        "8. fare: Fare\n",
        "9. cabin: Cabin number\n",
        "10. embarked: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
        "\n",
        "Let's start with some warmup exercises, which will go over Python concepts that are useful for our implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5zBgF3DJF1Z"
      },
      "source": [
        "# Warmup A: Break, Continue\n",
        "\n",
        "In a `for` or `while` loop:\n",
        "*   The `break` statement terminates the loop that contains it\n",
        "*   The `continue` statement skips the rest of the code for the current iteration, and continues onto the next iteration\n",
        "\n",
        "Note: if used in a nested loop, `break` and `continue` only affect the innermost loop!\n",
        "\n",
        "Examples: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t48UHSAvXTMY",
        "outputId": "fbc9a8ee-f8b4-483d-983e-dabf47741a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "hello = \"Hello World\"\n",
        "\n",
        "print(\"First word:\")\n",
        "for c in hello:\n",
        "    if c == \" \":\n",
        "        break\n",
        "    print(c)\n",
        "\n",
        "print(\"\\nNo vowels:\")\n",
        "for c in hello:\n",
        "    if c in ['a', 'e', 'i', 'o', 'u']:\n",
        "        continue\n",
        "    print(c)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word:\n",
            "H\n",
            "e\n",
            "l\n",
            "l\n",
            "o\n",
            "\n",
            "No vowels:\n",
            "H\n",
            "l\n",
            "l\n",
            " \n",
            "W\n",
            "r\n",
            "l\n",
            "d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBJjTeyREnit"
      },
      "source": [
        "## Try it\n",
        "Using `break` and `continue`, write a function `wacky_sum` that scans an array of integers from left to right, taking a running total of elements that are **not** a multiple of 10, and stopping when encountering a negative element"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BciAzeQN7nfZ",
        "outputId": "a0c53b81-2ff4-466d-c3b8-522f712aaaf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def wacky_sum(nums):\n",
        "    tot = 0\n",
        "    for num in nums:\n",
        "        if num < 0:\n",
        "            break\n",
        "        if num % 10 == 0:\n",
        "            continue\n",
        "        tot += num\n",
        "    return tot\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=6)\"\n",
        "import doctest\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  >>> print(wacky_sum([]))\n",
        "  0\n",
        "  >>> print(wacky_sum([-1]))\n",
        "  0\n",
        "  >>> print(wacky_sum([200]))\n",
        "  0\n",
        "  >>> print(wacky_sum([8, 7, 7, 5, 10, 6, -8, 5]))\n",
        "  33\n",
        "  >>> print(wacky_sum([0, 5, 10, 15, 20, 25, 30, -35, 40, -45, 50]))\n",
        "  45\n",
        "  >>> print(wacky_sum([2017, 2018, 2019, -2020, 2021, -2022]))\n",
        "  6054\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP5qbQyVCYR1"
      },
      "source": [
        "# Warmup B: List Comprehension\n",
        "A list comprehension is a way to define a new list or 2D array based on an existing list, as a more elegant alternative to using an explicit `for` loop\n",
        "\n",
        "A traditional `for` loop would have the format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRvlDGT3Hr2r",
        "outputId": "1feaa559-c36e-4a60-82c0-ec6ce07544b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "list2 = []\n",
        "for (item) in list1:\n",
        "    if (conditional):\n",
        "        list2.append((operation))\n",
        "return list2\n",
        "'''"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlist2 = []\\nfor (item) in list1:\\n    if (conditional):\\n        list2.append((operation))\\nreturn list2\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0Aki-oH4zB"
      },
      "source": [
        "While a list comprehension would have the format\n",
        "\n",
        "`list2 = [(operation) for (item) in list1 if (conditional)]`\n",
        "\n",
        "Note that the if conditional is optional!\n",
        "\n",
        "For example, if we wanted to find the lengths of all the elements in a list that start with 'a', we could use the following list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ExNc6hMhlh",
        "outputId": "34253778-076b-4425-e4c9-a5bc14b0c927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "animals = ['anteater', 'quail', 'moose', 'ape', 'dolphin']\n",
        "lens = [len(str) for str in animals if str[0] == 'a']\n",
        "lens"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJdzqYCRQhpK"
      },
      "source": [
        "### Try it\n",
        "Write a function `apply_evenly()` that takes in a function `func` and applies it to every other element (all even-indexed elements) in the list `list1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t7egvJnUYFc",
        "outputId": "743e676b-b1fc-464a-da2e-b6d90dc46958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def apply_evenly(func, list1):\n",
        "    return [func(list1[i]) for i in range(0, len(list1)) if i % 2 == 0]\n",
        "\n",
        "    #alternative solution: return [func(list1[i]) for i in range(0, len(list1), 2)]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=5)\"\n",
        "import doctest\n",
        "\n",
        "rand = [8, 5, 5, 1, 2, 3]\n",
        "langs = ['Python', 'Java', 'C', 'C++', 'HTML', 'Javascript']\n",
        "\n",
        "def lower2(str):\n",
        "    return str.lower()\n",
        "\n",
        "def sq(x):\n",
        "    return x ** 2\n",
        "\n",
        "def spread4(x):\n",
        "    return (np.ones(4) * x).tolist()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  >>> print(apply_evenly(len, langs))\n",
        "  [6, 1, 4]\n",
        "  >>> print(apply_evenly(len, animals))\n",
        "  [8, 5, 7]\n",
        "  >>> print(apply_evenly(lower2, langs))\n",
        "  ['python', 'c', 'html']\n",
        "  >>> print(apply_evenly(sq, rand))\n",
        "  [64, 25, 4]\n",
        "  >>> print(np.array(apply_evenly(spread4, rand)))\n",
        "  [[8. 8. 8. 8.]\n",
        "   [5. 5. 5. 5.]\n",
        "   [2. 2. 2. 2.]]\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJuinCUeuB5s"
      },
      "source": [
        "# Warmup C: Multidimensional Select\n",
        "\n",
        "Suppose we have a data matrix `wellness` that contains results of a wellness survey on students, with the following feature labels (in order):\n",
        "*  name\n",
        "*  sleep: the hours of sleep the student got the previous night\n",
        "*  meals: how many meals the student ate the previous day\n",
        "*  fruit: whether the student ate any fruit the previous day (1 for \"yes\", 0 for \"no\")\n",
        "\n",
        "Note that each row represents one student (sample point), and each column represents a feature!\n",
        "\n",
        "We can use the row and column index to select or assign elements in a 2D Numpy ndarray (eg. a data matrix):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yybDj3ooETYD",
        "outputId": "49a41c43-6e60-47d9-aee0-d3c8bc51bbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "wellness = np.array([[\"Alice\", 9, 3, 1], [\"Bob\", 7, 3, 0], [\"Sam\", 8, 3, 1], [\"Tina\", 6, 2, 1]])\n",
        "feature_labels = [\"name\", \"sleep\", \"meals\", \"fruit\"]\n",
        "print(\"Our ndarray: \\n\", wellness)\n",
        "\n",
        "#select from row 0, column 3: \n",
        "elem = wellness[0, 3]\n",
        "print(\"\\nDid Alice have fruit yesterday?\\n\", elem)\n",
        "\n",
        "#update row 2, column 1\n",
        "wellness[2, 1] = 9\n",
        "print(\"\\nSam says he actually slept 9 hours yesterday: \\n\", wellness)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our ndarray: \n",
            " [['Alice' '9' '3' '1']\n",
            " ['Bob' '7' '3' '0']\n",
            " ['Sam' '8' '3' '1']\n",
            " ['Tina' '6' '2' '1']]\n",
            "\n",
            "Did Alice have fruit yesterday?\n",
            " 1\n",
            "\n",
            "Sam says he actually slept 9 hours yesterday: \n",
            " [['Alice' '9' '3' '1']\n",
            " ['Bob' '7' '3' '0']\n",
            " ['Sam' '9' '3' '1']\n",
            " ['Tina' '6' '2' '1']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIdzEjnMIvvd"
      },
      "source": [
        "We can also select an entire row or column by index to get a 1D array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0PR4uyPMpgT",
        "outputId": "584ba4c0-b39f-405e-d1b9-c2e6d1d50dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#select row 3\n",
        "row3 = wellness[3, :]\n",
        "print(\"Tina: \\n\", row3)\n",
        "\n",
        "#select column 1\n",
        "col1 = wellness[:, 1]\n",
        "print(\"sleep: \\n\", col1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tina: \n",
            " ['Tina' '6' '2' '1']\n",
            "sleep: \n",
            " ['9' '7' '9' '6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmtEws6IOpch"
      },
      "source": [
        "Using a condition in the select statement will give us a filtered 2D array:\n",
        "*   Conditional format: \n",
        "`(table)[(table)[:, (column)] (comparison) (string)]`\n",
        "  *  You can have multiple conditional clauses: `(table)[clause1 * clause2 + clause3 ...]`, where * is the AND operator and + is the OR operator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR1rtCnkVxxB",
        "outputId": "66f385ef-1ebf-45f1-8dcf-49ece6fbd51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#select rows where column 1 is less than '8'\n",
        "lessthan8 = wellness[wellness[:, 1] < '8']\n",
        "print(\"Less than 8 hours of sleep: \\n\", lessthan8)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Less than 8 hours of sleep: \n",
            " [['Bob' '7' '3' '0']\n",
            " ['Tina' '6' '2' '1']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiG9lSpMdu0e"
      },
      "source": [
        "The `shape()` function of a ndarray gives us a tuple of array dimensions! Thus, for a 2D array:\n",
        "*  `shape[0]` gives us the number of rows (sample points)\n",
        "*  `shape[1]` gives us the number of columns (features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OjxfX-KemBK",
        "outputId": "a74ad7e9-6c84-40a9-e12b-7486871f7cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"Number of students:\", wellness.shape[0])\n",
        "print(\"Number of features:\", wellness.shape[1])\n",
        "print(\"\\n\")\n",
        "\n",
        "#print all names\n",
        "for i in range(0, wellness.shape[0]):\n",
        "    print(wellness[i, 0])\n",
        "print(\"\\n\")\n",
        "\n",
        "#print all feature labels\n",
        "for j in range(0, wellness.shape[1]):\n",
        "    print(feature_labels[j])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of students: 4\n",
            "Number of features: 4\n",
            "\n",
            "\n",
            "Alice\n",
            "Bob\n",
            "Sam\n",
            "Tina\n",
            "\n",
            "\n",
            "name\n",
            "sleep\n",
            "meals\n",
            "fruit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoOuy_A5hbdf"
      },
      "source": [
        "### Try it\n",
        "Use a select statement to find the names of all the students who had at least 3 meals yesterday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33o4Wtjo-6q4",
        "outputId": "c2bb519a-6573-4293-c878-351da002e75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "names = ...\n",
        "names = wellness[wellness[:, 2] >= '3'][:, 0]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=1)\"\n",
        "import doctest\n",
        "\"\"\"\n",
        "  >>> print(names)\n",
        "  ['Alice' 'Bob' 'Sam']\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vchq93XqT6La"
      },
      "source": [
        "# Warmup D: Information Gain Calculation\n",
        "\n",
        "Recall the equations for calculating information gain:\n",
        "\n",
        "![entropy after split.png](data:image/png;base64,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)\n",
        "\n",
        "![information gain equation.png](data:image/png;base64,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)\n",
        "\n",
        "Where:\n",
        "*  S<sub>left</sub> is the set of class labels belonging to the left child post-split, represented by `left` below\n",
        "*  S<sub>right</sub> is the set of class labels belonging to the right child post-split, represented by `right` below\n",
        "*  S is the set of all class labels belonging to the current node, represented by `y` below\n",
        "*  |s| is the number of elements in **any** set of class labels s— for our purposes, find |s| using the `.size` attribute of `numpy.ndarray` (the three sets mentioned above will be represented by ndarrays)\n",
        "*  H(s) is the entropy of **any** set of class labels s— which, for now, will be found by calling the function `entropy0` on s\n",
        "*  The `-, +, *, /` operators are also useful, as are parenthesis for order of operation\n",
        "\n",
        "Fill in the information gain calculation within `igcalc()` by combining the two equations above; this calculation will be used later in our `DecisionTree` implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxAy2e2OUXoY",
        "outputId": "00f31335-9a5e-47a9-b21e-c6b6949bd023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#This is actually the gini impurity function, which has similar behavior to \n",
        "#the entropy function, and is sometimes used in place of entropy in decision\n",
        "#trees; however, we will soon implement the actual entropy function\n",
        "def entropy0(labels, base=None):\n",
        "    value,counts = np.unique(labels, return_counts=True)\n",
        "    eq = 0\n",
        "    for cnt in counts:\n",
        "        p_i = cnt / labels.size\n",
        "        eq += p_i * (1 - p_i)\n",
        "    return eq\n",
        "\n",
        "def igcalc (left, right, y):\n",
        "    calc = ...\n",
        "    calc = entropy0(y) - (left.size * entropy0(left) + right.size * entropy0(right)) / (left.size + right.size)\n",
        "    return calc\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=6)\"\n",
        "import doctest\n",
        "\n",
        "left1, right1, y1 = np.array([0, 1]), np.array([0, 1]), np.array([0, 1, 0, 1])\n",
        "left2, right2, y2 = np.array([0, 0]), np.array([1, 1]), np.array([0, 1, 0, 1])\n",
        "\n",
        "left3, right3, y3 = np.array([0, 0, 1, 0]), np.array([1, 1, 1, 2]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "left4, right4, y4 = np.array([0, 0, 2]), np.array([1, 0, 1, 1, 1]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "\n",
        "left5, right5, y5 = np.array([0, 0, 0]), np.array([1, 1, 1, 1, 2]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "left6, right6, y6 = np.array([0, 0, 0, 2]), np.array([1, 1, 1, 1]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "\n",
        "\"\"\"\n",
        "  >>> print(igcalc(left1, right1, y1))\n",
        "  0.0\n",
        "  >>> print(igcalc(left2, right2, y2))\n",
        "  0.5\n",
        "  >>> print(igcalc(left3, right3, y3))\n",
        "  0.21875\n",
        "  >>> print(igcalc(left4, right4, y4))\n",
        "  0.2270833333333333\n",
        "  >>> print(igcalc(left5, right5, y5))\n",
        "  0.39375\n",
        "  >>> print(igcalc(left6, right6, y6))\n",
        "  0.40625\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz40rs9S2lVM"
      },
      "source": [
        "# Helpful Functions\n",
        "We are now ready to implement our `DecisionTree` class! Each section below will walk through the implementation of a function or method, making references to the following simple NumPy and SciPy functions as needed:\n",
        "*   ndarray transpose https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html\n",
        "*ndarray shape https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html\n",
        "*   numpy hstack https://numpy.org/doc/stable/reference/generated/numpy.hstack.html\n",
        "*   numpy unique https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
        "*   numpy where https://numpy.org/doc/stable/reference/generated/numpy.where.html\n",
        "*numpy linspace https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\n",
        "*numpy nan_to_num https://numpy.org/doc/stable/reference/generated/numpy.nan_to_num.html\n",
        "*numpy ones, zeros https://numpy.org/doc/stable/reference/generated/numpy.ones.html https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\n",
        "*  numpy unravel_index https://numpy.org/doc/stable/reference/generated/numpy.unravel_index.html\n",
        "*   collections.Counter and most_common() https://docs.python.org/2/library/collections.html\n",
        "*   scipy.stats.mode https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
        "  *   scipy ModeResult https://www.kite.com/python/docs/scipy.stats.mstats_basic.ModeResult\n",
        "*   scipy.stats.entropy https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIXeRDpbH87h"
      },
      "source": [
        "# Part A: preprocessing\n",
        "A function to preprocess our data matrix into something our decision tree can use\n",
        "\n",
        "Parameters:\n",
        "*   `data` is a numpy ndarray: the **full** data matrix with sample points as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `fill_mode` is a boolean (default True) for whether we want to fill in each missing value with the mode of its column\n",
        "  *  **Here, we are inferring the missing values of a column based on other values of the column; we use the mode of non-missing values because it works for both quantitative and categorical variables**\n",
        "*   `min_freq` is an integer (default 10): threshold for the minimum number of times a category must appear before we'd want to one-hot encode it into a binary variable\n",
        "*   `onehot_cols` is an array with the indeces of all the categorical variables in `data` to one-hot encode\n",
        "\n",
        "Fill in the `preprocess()` function as described:\n",
        "\n",
        "\n",
        "1.   Assign missing values (look for values that are `b''`) in the `data` table to `b'-1'` temporarily\n",
        "2.   Declare `onehot_encoding` and `onehot_features` to be empty arrays\n",
        "3.   Iterate through every `col` in `onehot_cols`\n",
        "\n",
        "  a. Pull column `col` from `data` (recall that `onehot_cols` is an array of column indices), and use it to intialize a `Counter()` called `counter`\n",
        "\n",
        "  b. Iterate through every `term` (`(category, count)` pair), in descending order by count (hint: check the `most_common()` method of `counter`)\n",
        "\n",
        "  *   Check if the `term`'s category represents a missing value (hint: what did you replace missing values with earlier?) if so, continue to next iteration\n",
        "  *   Check if the `term`'s count is below our threshold; if so, break the loop\n",
        "  *   Append `term`'s category to `onehot_features`, and a binary array to `onehot_encoding`, as described in the one-hot encoding process (hint: check `data`'s `col`-th column to get a boolean array, then cast it as an array of floats to get a binary array)\n",
        "\n",
        "  c. set `data`'s `col`-th column to `'0'` as it won't be used anymore\n",
        "4.   Convert `onehot_encoding` to a numpy array and transpose it; our binary arrays are now binary columns\n",
        "5.   Cast `data` as a numpy array of floats, then combine its columns with those of `onehot_encoding` (hint: check the `hstack()` function of numpy)\n",
        "6.   Check if `fill_mode` is enabled; if so, iterate through every column index `i` of `data`\n",
        "  \n",
        "  a.   Find the mode of column `i` for all non-missing values of the column; use the condition `(data[:, i] < -1 - eps) + (data[:, i] > -1 + eps)` to select rows of `data` where column `i` is **not** missing, then extract column `i` from the resulting table (hint: use `stats.mode()` from scipy, extract the mode from the ModeResult that it returns, and take element 0)\n",
        "  \n",
        "  b.   Assign the mode to **missing** values of column `i`; use a similar process as above to select missing values, but use the condition `(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)` instead\n",
        "7.   return `data, onehot_features`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqHwX_0RGpDJ"
      },
      "source": [
        "# Part B: DecisionTree constructor\n",
        "\n",
        "Creates a decision tree node\n",
        "\n",
        "Parameters:\n",
        "*   `max_depth` is an int (default 3) for how many levels of decision nodes we want in our current `DecisionTree` (a leaf would have `max_depth` 0)\n",
        "*   `feature_labels` is an array of strings (default None) for the names of our features\n",
        "\n",
        "Initialize the following based on the arguments passed in:\n",
        "*   `self.max_depth`\n",
        "*   `self.features`\n",
        "\n",
        "Initialize the following as an empty decision tree node:\n",
        "*   `self.left, self.right`\n",
        "*   `self.split_idx, self.thresh`\n",
        "*   `self.data, self.labels, self.pred`\n",
        "\n",
        "Note that the first four will be assigned in decision nodes, while the last three will be assigned in leaf nodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I29H2xyPSfNa"
      },
      "source": [
        "# Part C: entropy\n",
        "Calculates the entropy from a set of class labels\n",
        "\n",
        "Parameters:\n",
        "*   `labels` is a list of class labels (corresponding to points)\n",
        "*   `base` is an int (default None): the base to be used for the log function within entropy (None corresponds to natural log in `stats.entropy()`)\n",
        "\n",
        "Fill in the `entropy1()` function as described:\n",
        "1.   Get the counts for each class in `labels` (hint: check the `unique()` function of numpy)\n",
        "2.   Return the entropy calculated on the counts; you may use `stats.entropy() ` from scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kr6ot-PZBUm"
      },
      "source": [
        "# Part D: information gain\n",
        "Calculates the information gain from splitting points on feature column `X` and value `thresh`\n",
        "\n",
        "Parameters:\n",
        "*   `X` is an array of numeric values: the column of our data matrix that corresponds to the feature that we want to try splitting on\n",
        "*   `y` is an array of class labels (corresponding to points in data matrix)\n",
        "*   `thresh` is a numeric value: the value that we want to try splitting on\n",
        "\n",
        "Fill in the `information_gain()` function as described:\n",
        "1.   Find the two lists of **class labels** for the left and right children after the split, and create numpy arrays from these lists (hint: use list comprehensions with conditions `X[i] < thresh`, `X[i] >= thresh` as conditions)\n",
        "2.   Calculate the information gain from the left and right class labels like in **the warmup**, but with `entropy1()` in place of `entropy0()`\n",
        "3.   Return the information gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qaKRdqzOwqI"
      },
      "source": [
        "# Part E: split test\n",
        "Given a feature and split value, splits sample points and sample point indices in two\n",
        "\n",
        "Parameters:\n",
        "\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `idx` is an int: the column index of the feature that we want to split on\n",
        "*   `thresh` is a numeric value: the value that we want to split on\n",
        "\n",
        "Fill in the `split_test()` function as described:\n",
        "1.   Define `idx0`, `idx1` to be sets of sample point indices (rows in `X`) for the left and right children: ie. one where the values of feature `idx` are less than `thresh`, and another where they are greater than or equal to `thresh` (hint: check the `where()` function of numpy; provide a condition only and get the 0-th element of the function call)\n",
        "2.   Using the two sets of indices above, split `X` into two ndarrays `X0`, `X1`: one for the left child, and one for the right child\n",
        "3.   Return `X0`, `idx0`, `X1`, `idx1` in that order\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VADyeCCcT9Y-"
      },
      "source": [
        "# Part F: split\n",
        "Splits a set of data points **and** a set of class labels in two, given a feature and split value\n",
        "\n",
        "Parameters:\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `y` is a list of class labels (corresponding to points)\n",
        "*   `idx` is an int: the column index of the feature that we want to split on\n",
        "*   `thresh` is a numeric value: the value that we want to split on\n",
        "\n",
        "Fill in the `split()` function as described:\n",
        "1.   Call `split_test()` to get `X0`, `idx0`, `X1`, `idx1` as described in the previous part\n",
        "2.   Use `idx0`, `idx1` to split `y` into two lists `y0` and `y1`\n",
        "3.   Return `X0`, `y0`, `X1`, `y1` in that order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBbRevyWEDeA"
      },
      "source": [
        "# Part G: fit\n",
        "Recursively grows a decision tree given a set of data points and a set of class labels\n",
        "\n",
        "Parameters:\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `y` is a list of class labels (corresponding to points)\n",
        "\n",
        "Fill in the `fit()` function as described:\n",
        "\n",
        "1.   Check if the `self.max_depth` of the tree is greater than 0\n",
        "\n",
        "Fill in the following under the main `if` clause\n",
        "2.   Create an empty array `gains` to store calculated information gains\n",
        "3.   Use list comprehension to create a 2D array: for each feature index `j`, use linear interpolation with `num=10` to find a 1D array of split values we'd like to try for that feature (hint #1: you can use `linspace()` in numpy to create the 1D array) (hint #2: use `X.shape[1]` to get the total number of features) \n",
        "\n",
        "*   Linear interpolation in this case: Take the minimum and maximum sample values of column `j`, x<sub>j-min</sub> and x<sub>j-max</sub>, and try `num` evenly spaced numbers over the interval [x<sub>j-min</sub> + eps, x<sub>j-max</sub> - eps] where eps is a very small number (hint #1: use `np.min()` and `np.max()`) (hint #2: `eps` is already defined)\n",
        "*   Finally, create a numpy array from the 2D array and assign it to `thresh_list`\n",
        "4.  For each feature index `j`:\n",
        "\n",
        "  a.  Use list comprehension to create a 1D array: for each value `t` in row `j` of `thresh_list`, call `information_gain()` on column `j` of `X`, `y`, and `t`\n",
        "\n",
        "  b.  Append the 1D array to `gains`\n",
        "5.  Convert `gains` to a numpy array, and call `np.nan_to_num()` on `gains` to replace NaN values with 0.0\n",
        "6.  Find the feature index and index of the threshold value that corresponds to the maximum information gain in `gains`, and assign them to `self.split_idx, thresh_idx`\n",
        "*   `gains` is a 2D array, but `np.argmax()` returns the 1D coordinate of the maximum gain along a **flattened** array! We need the 2D coordinates from `gain`, which can be found using `np.unravel_index(np.argmax(gains), gains.shape)`\n",
        "7.  Use `self.split_idx, thresh_idx` to find the threshold value in `thresh_list` that corresponds to the maximum gain; assign the value to `self.thresh` (hint: `thresh_list` is indexed the same way as `gains`)\n",
        "8.  Call `split()` to split our data point set and class label set in two: `X0, y0, X1, y1`\n",
        "9.  Using an `if` statement to check that both `X0` and `X1` are not empty: (this will dictate that the current node be a decision node)\n",
        "\n",
        "  a.  Construct two `DecisionTrees`, one for the left and one for the right child, and assign them to `self.left` and `self.right` (hint: what should you pass into the constructor for `max_depth` and `feature_labels`? How deep do you want a child to be in relation to `self.max_depth`?)\n",
        "\n",
        "  b.  Make a recursive call to `fit()`, for both `self.left` and `self.right`, passing in their respective data point sets and class label sets\n",
        "10.  Under `else`: (this will dictate a leaf node since either `X0`or `X1` is empty; we cannot split further)\n",
        "\n",
        "  a.  Set `self.max_depth` to 0\n",
        "\n",
        "  b.  Store `X, y` in `self.data, self.labels`\n",
        "\n",
        "  c.  Assign `self.pred`, the final prediction of the node, to the mode of `y` (hint: use `stats.mode()` from scipy, extract the mode from the ModeResult that it returns, and take element 0)\n",
        "\n",
        "Fill in the following under the main `else` clause (this will dictate a leaf node since we have reached the max depth)\n",
        "11.  As above, store `X, y` in `self.data, self.labels`, then assign `self.pred`, the final prediction of the node, to be the mode of `y`\n",
        "\n",
        "Finally, outside of the `if-else` statements, return the current node `self`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQWUWgsFdfSD"
      },
      "source": [
        "# Part H: predict\n",
        "Recursively traverses a decision tree to predict class labels for a given set of data points\n",
        "\n",
        "Parameters:\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "\n",
        "Fill in the `predict()` function as described:\n",
        "1.  Check if `self` is a leaf node (what would `max_depth` be?)\n",
        "  \n",
        "  a.  Return a 1D numpy array predicting `self.pred` for each sample point (hint #1: use the `*` operator to broadcast `self.pred` onto an array of ones the size of n, where n is the number of sample points) (hint #2: check `ones()` of numpy)\n",
        "2.  Else\n",
        "\n",
        "  a.  Call `split_test()` to split our data point set and index set in two: `X0, idx0, X1, idx1`\n",
        "\n",
        "  b.  Define `yhat` to be a 1D numpy array with a zero for each sample point (hint: check `zeros()` of numpy)\n",
        "\n",
        "  c.  Assign the values of `yhat` indexed by `idx0`, by recursively predicting on `X0` using the left child node\n",
        "  \n",
        "  d.  Repeat for `idx1` and `X1` using the right child node\n",
        "\n",
        "  e.  Return the node's predictions `yhat`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aie0fazLVYOq"
      },
      "source": [
        "class DecisionTree:\n",
        "    #Part B: Creates a decision tree node\n",
        "    def __init__(self, max_depth=3, feature_labels=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.features = feature_labels\n",
        "        # for non-leaf nodes\n",
        "        self.left, self.right = None, None\n",
        "        # for non-leaf nodes\n",
        "        self.split_idx, self.thresh = None, None\n",
        "        # for leaf nodes\n",
        "        self.data, self.labels, self.pred = None, None, None\n",
        "\n",
        "    #Part C: Calculates the entropy from a set of class labels\n",
        "    global entropy1\n",
        "    def entropy1(labels, base=None):\n",
        "        #get counts for each class label in labels; find and return entropy\n",
        "        value,counts = np.unique(labels, return_counts=True)\n",
        "        return stats.entropy(counts, base=base)\n",
        "    \n",
        "    @staticmethod\n",
        "    #Part D: Calculates the information gain from splitting points on feature column X and value thresh\n",
        "    def information_gain(X, y, thresh):\n",
        "        #condition X against thresh to split y into left and right child sets\n",
        "        left = np.array([y[i] for i in range(0, y.size) if X[i] < thresh])\n",
        "        right = np.array([y[i] for i in range(0, y.size) if X[i] >= thresh])\n",
        "        #calculate and return information gain from equation\n",
        "        gain = entropy1(y) - (left.size * entropy1(left) + right.size * entropy1(right)) / (left.size + right.size)\n",
        "        return gain\n",
        "\n",
        "    #Part F: Splits a set of data points and a set of class labels in two, given a feature and split value\n",
        "    def split(self, X, y, idx, thresh):\n",
        "        #call split_test, then use the two sets of indices to split y into two sets\n",
        "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
        "        y0, y1 = y[idx0], y[idx1]\n",
        "        #return X0, y0, X1, y1\n",
        "        return X0, y0, X1, y1\n",
        "\n",
        "    #Part E: Given a feature and split value, splits sample points and sample point indices in two\n",
        "    def split_test(self, X, idx, thresh):\n",
        "        #get two sets of sample point indices by conditioning column idx of X against thresh\n",
        "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
        "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
        "        #split X into two sets using the sample point indices above\n",
        "        #return X0, idx0, X1, idx1\n",
        "        X0, X1 = X[idx0, :], X[idx1, :]\n",
        "        return X0, idx0, X1, idx1\n",
        "\n",
        "    #Part G: Recursively grows a decision tree given a set of data points and a set of class labels\n",
        "    def fit(self, X, y):\n",
        "        #check self.max_depth\n",
        "        if self.max_depth > 0:\n",
        "            #create gains array\n",
        "            gains = []\n",
        "            #build thresh_list: values to try for each feature thresholding with a linear interpolation of 10 values; \n",
        "            #including logic to prevent thresholding on exactly the minimum or maximum values\n",
        "            #which may not lead to any meaningful node splits\n",
        "            thresh_list = np.array([\n",
        "                np.linspace(np.min(X[:, j]) + eps, np.max(X[:, j]) - eps, num=10)\n",
        "                for j in range(X.shape[1])\n",
        "            ])\n",
        "            #compute info gain for all single-dimension splits; convert to np array and remove NaNs\n",
        "            for j in range(X.shape[1]):\n",
        "                gains.append([self.information_gain(X[:, j], y, t) for t in thresh_list[j, :]])\n",
        "            gains = np.nan_to_num(np.array(gains))\n",
        "            #find optimal feature index and index of threshold value using argmax and unravel_index\n",
        "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
        "            #extract the optimal threshold value from thresh_list\n",
        "            self.thresh = thresh_list[self.split_idx, thresh_idx]\n",
        "            #call split() to get two sets of data pts and class labels\n",
        "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
        "            #check that neither X0 and X1 are empty (decision node): recursively build and fit left/right children\n",
        "            if X0.size > 0 and X1.size > 0:\n",
        "                self.left = DecisionTree(\n",
        "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
        "                self.left.fit(X0, y0)\n",
        "                self.right = DecisionTree(\n",
        "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
        "                self.right.fit(X1, y1)\n",
        "            #else (leaf node): set max_depth to 0, store X and y, and assign mode of y to be the final pred\n",
        "            else:\n",
        "                self.max_depth = 0\n",
        "                self.data, self.labels = X, y\n",
        "                self.pred = stats.mode(y).mode[0]\n",
        "        #else (leaf node): max_depth is 0, store X and y, and assign mode of y to be the final pred\n",
        "        else:\n",
        "            self.data, self.labels = X, y\n",
        "            self.pred = stats.mode(y).mode[0]\n",
        "        #return current node\n",
        "        return self\n",
        "\n",
        "    #Part H: Recursively traverses a decision tree to predict class labels for a given set of data points\n",
        "    def predict(self, X):\n",
        "        #check if leaf node: return pred for all sample pts\n",
        "        if self.max_depth == 0:\n",
        "            return self.pred * np.ones(X.shape[0])\n",
        "        #else (decision node): call split_test() to split into two sets of sample pts and sample pt indices;\n",
        "        #create yhat = array of 0's;\n",
        "        #recursively predict on the two sets using the left and right children and insert into yhat\n",
        "        else:\n",
        "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
        "            yhat = np.zeros(X.shape[0])\n",
        "            yhat[idx0] = self.left.predict(X0)\n",
        "            yhat[idx1] = self.right.predict(X1)\n",
        "            #return all predictions\n",
        "            return yhat\n",
        "        \n",
        "#Part A: A function to preprocess our data matrix into something our decision tree can use\n",
        "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
        "    # Temporarily assign -1 to missing data\n",
        "    data[data == b''] = b'-1'\n",
        "\n",
        "    # Hash the columns (used for handling strings)\n",
        "    onehot_encoding = []\n",
        "    onehot_features = []\n",
        "    for col in onehot_cols:\n",
        "        #initialize a Counter from data column, to find the most common categorical terms and their frequencies\n",
        "        counter = Counter(data[:, col])\n",
        "        for term in counter.most_common():\n",
        "            #ignore missing terms and only encode category if frequency is above threshold\n",
        "            if term[0] == b'-1':\n",
        "                continue\n",
        "            if term[-1] <= min_freq:\n",
        "                break\n",
        "            #keep track of feature-column correspondence, then one-hot encode into binary arrays\n",
        "            #neutralize original column\n",
        "            #transpose to get binary columns and add them to data matrix\n",
        "            onehot_features.append(term[0])\n",
        "            onehot_encoding.append((data[:, col] == term[0]).astype(np.float))\n",
        "        data[:, col] = '0'\n",
        "    onehot_encoding = np.array(onehot_encoding).T\n",
        "    data = np.hstack([np.array(data, dtype=np.float), np.array(onehot_encoding)])\n",
        "\n",
        "    # Replace missing data with the mode value. We use the mode instead of\n",
        "    # the mean or median because this makes more sense for categorical\n",
        "    # features such as gender or cabin type, which are not ordered.\n",
        "    if fill_mode:\n",
        "        for i in range(data.shape[-1]):\n",
        "            #get values of column i that are not close to -1\n",
        "            #get the mode of the above using scipy.stats.mode and accessing ModeResult\n",
        "            #assign mode to values of column i that are close to -1\n",
        "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
        "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
        "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
        "    #return data, onehot_features\n",
        "    return data, onehot_features\n",
        "\n",
        "\n",
        "def evaluate(clf):\n",
        "    print(\"Cross validation\", sklearn.model_selection.cross_val_score(clf, X, y))\n",
        "    if hasattr(clf, \"decision_trees\"):\n",
        "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
        "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
        "        print(\"First splits\", first_splits)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef-4GLLdd_La"
      },
      "source": [
        "# Preprocessing\n",
        "After implementing all the decision tree functions, run the cell below to preprocess our Titanic training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1joGH4W0VZMW",
        "outputId": "1db7ae60-093c-4a22-a2c4-94cb48141e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#import titanic dataset files as training and test data\n",
        "path_train = '/content/drive/My Drive/Colab Notebooks/decision trees/titanic_training.csv'\n",
        "data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
        "\n",
        "path_test = '/content/drive/My Drive/Colab Notebooks/decision trees/titanic_testing_data.csv'\n",
        "test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
        "\n",
        "#define our set of class labels from imported test data (0 = Died, 1 = Survived)\n",
        "y_titanic = data[1:, 0]\n",
        "class_names_titanic = [\"Died\", \"Survived\"]\n",
        "\n",
        "#for class labels AND preprocessed data, use labeled_idx to keep only rows where class labels are present\n",
        "labeled_idx = np.where(y_titanic != b'')[0]\n",
        "y_titanic = np.array(y_titanic[labeled_idx], dtype=np.int)\n",
        "\n",
        "#preprocess training data and test data\n",
        "print(\"Preprocessing the titanic dataset\")\n",
        "X_titanic, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
        "X_titanic = X_titanic[labeled_idx, :]\n",
        "Z_titanic, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
        "\n",
        "#check that our two sets have same # of columns\n",
        "#define list of feature names\n",
        "assert X_titanic.shape[1] == Z_titanic.shape[1]\n",
        "features_titanic = list(data[0, 1:]) + onehot_features"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing the titanic dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmLQoG9f83HW"
      },
      "source": [
        "# Before using our `DecisionTree`:\n",
        "\n",
        "\n",
        "*   Split our training data into training and validation sets\n",
        "  *  Randomly assign 20% of data to validation set, keeping 80% in training set; this is commonly called the \"80/20\" split\n",
        "*  Print the names of our features, before and after the preprocessing of our data matrices\n",
        "  *  Recall that we one-hot encoded while preprocessing!\n",
        "*  Run a \"dummy\" constant classifier on training set, which predicts 0 (\"didn't survive\") for each passenger\n",
        "  *  We'll refer back to this as a baseline accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IKPNLHXVgco",
        "outputId": "f791a407-e86e-44f2-d2d0-f165d1d049f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#split training into 80% training, 20% validation; use random state to keep it deterministic for our purposes\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_titanic, y_titanic, test_size=0.2, random_state=49)\n",
        "\n",
        "#print train, test, train/val sizes\n",
        "print(\"Total training size:\", X_titanic.shape[0])\n",
        "print(\"Total test size:\", Z_titanic.shape[0])\n",
        "print(\"Training/validation sizes:\", X_train.shape[0], X_val.shape[0])\n",
        "\n",
        "#print our feature labels before and after one-hot encoding\n",
        "print(\"\\n\\nFeatures before one-hot encoding:\", list(data[0, 1:]))\n",
        "print(\"Features after one-hot encoding:\", features_titanic)\n",
        "\n",
        "#constant classifier which always predicts 0\n",
        "print(\"\\n\\nAccuracy of constant classifier:\", 1 - np.sum(y_titanic) / y_titanic.size)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training size: 999\n",
            "Total test size: 310\n",
            "Training/validation sizes: 799 200\n",
            "\n",
            "\n",
            "Features before one-hot encoding: [b'pclass', b'sex', b'age', b'sibsp', b'parch', b'ticket', b'fare', b'cabin', b'embarked']\n",
            "Features after one-hot encoding: [b'pclass', b'sex', b'age', b'sibsp', b'parch', b'ticket', b'fare', b'cabin', b'embarked', b'male', b'female', b'S', b'C', b'Q']\n",
            "\n",
            "\n",
            "Accuracy of constant classifier: 0.6136136136136137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWFtJXSfLUQC"
      },
      "source": [
        "# Training our `DecisionTree`\n",
        "We're finally ready to apply our `DecisionTree` to the training set! Run the cell below to:\n",
        "\n",
        "\n",
        "*   Create an instance of `DecisionTree` and grow it on our set of training points\n",
        "*  Use the `DecisionTree` to predict on the training and validation sets, and obtain the training and validation accuracies\n",
        "  *  The training accuracy is how well our decision tree is able to model the dataset that it was constructed on\n",
        "  *  The validation accuracy is how well it can generalize to predict on data that it hasn't seen yet\n",
        "*  Check that training and validation accuracies match the staff accuracies (which should be the case if `DecisionTree` was implemented as described)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88LZvgU5Gs47",
        "outputId": "c8479ee7-22bd-45e4-c055-844a73c53174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#create a DecisionTree instance and train it on the training set\n",
        "dt = DecisionTree(max_depth=8, feature_labels=features_titanic)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "#use it to predict on our training set and validation set\n",
        "train_pred = dt.predict(X_train)\n",
        "val_pred = dt.predict(X_val)\n",
        "train_acc = 1 - np.count_nonzero(train_pred - y_train) / y_train.size\n",
        "val_acc = 1 - np.count_nonzero(val_pred - y_val) / y_val.size\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=2)\"\n",
        "import doctest\n",
        "\"\"\"\n",
        "  >>> print(train_acc)\n",
        "  0.8660826032540676\n",
        "  >>> print(val_acc)\n",
        "  0.795\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.8660826032540676\n",
            "Validation Accuracy: 0.795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S_FfK9tQ6Ia"
      },
      "source": [
        "# Final Predictions\n",
        "Once we are satisfied with our validation accuracy, we can make our final predictions on the test set\n",
        "\n",
        "*   Now that validation is complete, we should train on our **entire** training set (including points previously assigned to validation) before making test predictions\n",
        "*   Predict on the test set, and ensure that the test predictions match the staff predictions\n",
        "  *  Recall that we don't have the true class labels for the test set, so we cannot obtain a test accuracy! However, a test accuracy would be used in practice as a final evaluation of the model's performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsP4J_I9GwHR",
        "outputId": "bc6b9fce-c0b1-42ee-e40d-dac1e6eda1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#train on entire training set\n",
        "dt.fit(X_titanic, y_titanic)\n",
        "test_pred = dt.predict(Z_titanic)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=1)\"\n",
        "import doctest\n",
        "\"\"\"\n",
        "  >>> print(test_pred)\n",
        "  [1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
        "   0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
        "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0.\n",
        "   1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
        "   1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n",
        "   1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
        "   0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
        "   0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
        "   0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
        "   0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
        "   0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
        "   1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
        "   0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVVO7bxbaT5l"
      },
      "source": [
        "# Bonus: Visualization\n",
        "\n",
        "*  Recall that we imported the `graphviz` module, which is a Python interface for the popular Graphviz graph visualization software\n",
        "*  In particular, we will be using the Digraph (directed graph) class from the module to represent a decision tree\n",
        "\n",
        "Run the following code to visualize the decision tree trained above— but limited to depth 3, for visibility\n",
        "\n",
        "*   The PDF file will show up in `/Colab Notebooks/decision trees/test-output/`\n",
        "*  It should look like: ![](https://drive.google.com/uc?export=view&id=1PE9UNo-df4d7KGacACFaiKOLWT9hOGBK)\n",
        "*  Each decision node shows the name of the feature (corresponding to `self.split_idx`), and the threshold value the feature is split on `self.thresh`\n",
        "  *  For example, in the decision node labeled \"b'fare', threshold 27.816...\", we are splitting on the \"fare\" feature using a threshold value ~27.816; this means that when predicting on a sample point, we will look at this node's left child if the point's \"fare\" is less than 27.816, or right child if the point's \"fare\" is greater than (or equal to) 27.816\n",
        "*  Each leaf node shows its final prediction `self.pred`\n",
        "  *  For example, in the far left leaf node labeled \"class: 1\", we will predict class 1 (survived) for any sample point that lands on this node\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4TXyyDwacJM",
        "outputId": "096fbb2a-32c8-4d5e-cddb-058212e2f55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tree_vis = Digraph(comment='Titanic decision tree')\n",
        "i = 0\n",
        "\n",
        "#recursive function that walks through a tree and builds the Digraph node by node (either threshold or class label)\n",
        "def walk(DT):\n",
        "    global i\n",
        "    #leaf node\n",
        "    if DT.max_depth == 0:\n",
        "        #nodenum = unique label for every node\n",
        "        nodenum = str(i)\n",
        "        tree_vis.node(nodenum, 'class: ' + str(DT.pred))\n",
        "        i += 1\n",
        "        return nodenum\n",
        "    #decision node; recursive calls\n",
        "    else:\n",
        "        #print feature to split on, and threshold\n",
        "        label = str(DT.features[DT.split_idx]) + ', threshold ' + str(DT.thresh)\n",
        "        nodenum = str(i)\n",
        "        i += 1\n",
        "        tree_vis.node(nodenum, label)\n",
        "        tree_vis.edge(nodenum, walk(DT.left))\n",
        "        tree_vis.edge(nodenum, walk(DT.right))\n",
        "        return nodenum\n",
        "\n",
        "#create and train the depth 3 DecisionTree\n",
        "dt3 = DecisionTree(max_depth=3, feature_labels=features_titanic)\n",
        "dt3.fit(X_train, y_train)\n",
        "\n",
        "walk(dt3)\n",
        "#un-comment to print the link between nodes\n",
        "#print(tree_vis.source)\n",
        "\n",
        "#delete existing output graph if exists\n",
        "#render graph\n",
        "try:\n",
        "    os.remove('test-output/titanic_3.gv')\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "tree_vis.render('/content/drive/My Drive/Colab Notebooks/decision trees/test-output/titanic_3.gv', view=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/decision trees/test-output/titanic_3.gv.pdf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}